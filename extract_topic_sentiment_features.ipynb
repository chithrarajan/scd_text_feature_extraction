{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Combine and Preprocess All Texts from Multiple VTT Files\n",
    "We'll start by reading all VTT files, extracting the text (cleaning and tokenizing), and preparing it for topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chithraanand/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import webvtt\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Load spaCy for NLP processing\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess and clean text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize and remove punctuation, stopwords\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if token.text.lower() not in stop_words and token.text not in string.punctuation]\n",
    "    return \" \".join(tokens)  # Return preprocessed text as a single string\n",
    "\n",
    "# Function to extract texts from multiple VTT files\n",
    "def extract_texts_from_vtt_files(file_paths):\n",
    "    conversation_texts = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # Parse each VTT file\n",
    "        for caption in webvtt.read(file_path):\n",
    "            speaker_text = caption.text.strip()  # Assuming speaker's text is stored here\n",
    "            if speaker_text:\n",
    "                # Remove speaker label (e.g., \"Speaker 1:\", \"Speaker 2:\")\n",
    "                cleaned_text = re.sub(r'^[A-Za-z0-9\\s]+?:\\s*', '', speaker_text)  # Regex to remove speaker labels\n",
    "                if cleaned_text:\n",
    "                    conversation_texts.append(cleaned_text)\n",
    "    \n",
    "    return conversation_texts\n",
    "\n",
    "# Example: Extracting texts from multiple VTT files\n",
    "file_paths = [\"HAKA3_copy.vtt\", \"meeting1_copy.vtt\", \"meeting2_copy.vtt\"]  # Replace with actual paths to your VTT files\n",
    "conversation_texts = extract_texts_from_vtt_files(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Apply LDA to the Entire Corpus\n",
    "Now, we can apply LDA to the combined corpus of texts from all the VTT files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: biology, minor, university, thank, student\n",
      "Topic 1: test, thank, student, good, biology\n",
      "Topic 2: biology, want, hope, really, yeah\n",
      "Topic 3: work, master, yeah, try, major\n",
      "Topic 4: illinois, good, right, okay, delhi\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Function to extract topics using LDA with scikit-learn\n",
    "def extract_topics_from_text(texts, num_topics=5, num_words=5):\n",
    "    # Preprocess texts: clean and tokenize\n",
    "    processed_texts = [preprocess_text(text) for text in texts]\n",
    "    \n",
    "    # Create a document-term matrix (DTM) using CountVectorizer\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    dtm = vectorizer.fit_transform(processed_texts)\n",
    "    \n",
    "    # Apply LDA (Latent Dirichlet Allocation) using scikit-learn\n",
    "    lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    lda_model.fit(dtm)\n",
    "    \n",
    "    # Get the top words for each topic\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topics = []\n",
    "\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        top_words_idx = topic.argsort()[:-num_words - 1:-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        topics.append(top_words)\n",
    "    \n",
    "    # Return the LDA model, document-term matrix, and the topics\n",
    "    return lda_model, dtm, topics\n",
    "\n",
    "# Example: Extract LDA topics from the entire corpus of conversation texts\n",
    "num_topics = 5  # Choose the number of topics you want to extract\n",
    "lda_model, dtm, topics = extract_topics_from_text(conversation_texts, num_topics=num_topics)\n",
    "\n",
    "# Display the top words for each topic\n",
    "for idx, topic in enumerate(topics):\n",
    "    print(f\"Topic {idx}: {', '.join(topic)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Get Topic Distribution for Each Document (VTT File Segment)\n",
    "Once the LDA model is trained on all the documents, we can get the topic distribution for each document (i.e., each conversation segment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.72975016 0.06669885 0.06667957 0.06667588 0.07019554]\n",
      " [0.02507588 0.02500599 0.0250024  0.02500172 0.89991402]\n",
      " [0.05073794 0.05002668 0.05001069 0.79921222 0.05001247]\n",
      " [0.83963979 0.04001525 0.04000612 0.04019613 0.04014272]\n",
      " [0.7327856  0.06668956 0.06667585 0.06667323 0.06717577]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to get the topic distribution for each document\n",
    "def get_topic_distribution_for_documents(lda_model, dtm, num_topics):\n",
    "    # Use the transform method of the LDA model to get topic distributions for each document\n",
    "    topic_distributions = lda_model.transform(dtm)\n",
    "    return topic_distributions\n",
    "\n",
    "# Example: Get topic distributions for all conversation segments\n",
    "topic_distributions = get_topic_distribution_for_documents(lda_model, dtm, num_topics)\n",
    "\n",
    "# Print the topic distributions for the first 5 documents (just as an example)\n",
    "print(topic_distributions[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Sentiment Analysis (TextBlob)\n",
    "Next, we perform sentiment analysis and get the polarity, subjectivity, and sentiment label (positive/negative/neutral) for each text segment:\n",
    "\n",
    "Polarity: A continuous score indicating sentiment (positive/negative).\n",
    "Subjectivity: A continuous score indicating how subjective (opinion-based) or objective the text is.\n",
    "Sentiment_Label: A categorical sentiment label (e.g., Positive, Negative, Neutral).\n",
    "\n",
    "Polarity > 0: Positive sentiment.\n",
    "Polarity < 0: Negative sentiment.\n",
    "Polarity = 0: Neutral sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Polarity  Subjectivity Sentiment_Label\n",
      "0  0.910000      0.780000        Positive\n",
      "1  0.512121      0.551515        Positive\n",
      "2  0.500000      0.500000        Positive\n",
      "3  0.000000      0.000000         Neutral\n",
      "4 -0.050000      0.050000        Negative\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Function to get sentiment features (polarity and subjectivity) for each text\n",
    "def get_sentiment_features(texts):\n",
    "    sentiments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        blob = TextBlob(text)\n",
    "        polarity = blob.sentiment.polarity   # Sentiment polarity (between -1 and 1)\n",
    "        subjectivity = blob.sentiment.subjectivity  # Sentiment subjectivity (between 0 and 1)\n",
    "        \n",
    "        sentiments.append([polarity, subjectivity])\n",
    "    \n",
    "    return sentiments\n",
    "\n",
    "# Function to categorize sentiment polarity into Positive, Neutral, Negative\n",
    "def categorize_sentiment(polarity):\n",
    "    if polarity > 0:\n",
    "        return \"Positive\"\n",
    "    elif polarity < 0:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Get sentiment features for all conversation segments\n",
    "sentiment_features = get_sentiment_features(conversation_texts)\n",
    "\n",
    "# Create a DataFrame for sentiment features\n",
    "sentiment_df = pd.DataFrame(sentiment_features, columns=[\"Polarity\", \"Subjectivity\"])\n",
    "\n",
    "# Apply categorization to the Polarity column\n",
    "sentiment_df[\"Sentiment_Label\"] = sentiment_df[\"Polarity\"].apply(categorize_sentiment)\n",
    "\n",
    "# Display the first few rows of sentiment features\n",
    "print(sentiment_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Combine Topic Features with Sentiment Features\n",
    "Now that you have both the topic features and sentiment features, we can combine them into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 0: [be, learn, see, say, psychology]</th>\n",
       "      <th>Topic 1: [right, say, psychology, first, be]</th>\n",
       "      <th>Topic 2: [be, still, go, minor, student]</th>\n",
       "      <th>Topic 3: [structure, kind, student, science, interested]</th>\n",
       "      <th>Topic 4: [good, first, move, lot, datum]</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Sentiment_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.729750</td>\n",
       "      <td>0.066699</td>\n",
       "      <td>0.066680</td>\n",
       "      <td>0.066676</td>\n",
       "      <td>0.070196</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.025076</td>\n",
       "      <td>0.025006</td>\n",
       "      <td>0.025002</td>\n",
       "      <td>0.025002</td>\n",
       "      <td>0.899914</td>\n",
       "      <td>0.512121</td>\n",
       "      <td>0.551515</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.050738</td>\n",
       "      <td>0.050027</td>\n",
       "      <td>0.050011</td>\n",
       "      <td>0.799212</td>\n",
       "      <td>0.050012</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.839640</td>\n",
       "      <td>0.040015</td>\n",
       "      <td>0.040006</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.040143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.732786</td>\n",
       "      <td>0.066690</td>\n",
       "      <td>0.066676</td>\n",
       "      <td>0.066673</td>\n",
       "      <td>0.067176</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.033724</td>\n",
       "      <td>0.033344</td>\n",
       "      <td>0.033493</td>\n",
       "      <td>0.033336</td>\n",
       "      <td>0.866102</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.100009</td>\n",
       "      <td>0.100021</td>\n",
       "      <td>0.100008</td>\n",
       "      <td>0.100006</td>\n",
       "      <td>0.599956</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.839255</td>\n",
       "      <td>0.040013</td>\n",
       "      <td>0.040588</td>\n",
       "      <td>0.040004</td>\n",
       "      <td>0.040141</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.910858</td>\n",
       "      <td>0.022230</td>\n",
       "      <td>0.022374</td>\n",
       "      <td>0.022275</td>\n",
       "      <td>0.022262</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.050008</td>\n",
       "      <td>0.050019</td>\n",
       "      <td>0.799153</td>\n",
       "      <td>0.050572</td>\n",
       "      <td>0.050248</td>\n",
       "      <td>-0.700000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.050321</td>\n",
       "      <td>0.050018</td>\n",
       "      <td>0.798881</td>\n",
       "      <td>0.050005</td>\n",
       "      <td>0.050775</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.067354</td>\n",
       "      <td>0.066688</td>\n",
       "      <td>0.732291</td>\n",
       "      <td>0.066673</td>\n",
       "      <td>0.066995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.016796</td>\n",
       "      <td>0.016672</td>\n",
       "      <td>0.933117</td>\n",
       "      <td>0.016713</td>\n",
       "      <td>0.016702</td>\n",
       "      <td>0.201389</td>\n",
       "      <td>0.473611</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.033724</td>\n",
       "      <td>0.033345</td>\n",
       "      <td>0.034437</td>\n",
       "      <td>0.033762</td>\n",
       "      <td>0.864733</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.011767</td>\n",
       "      <td>0.011770</td>\n",
       "      <td>0.011784</td>\n",
       "      <td>0.952905</td>\n",
       "      <td>0.011775</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.100007</td>\n",
       "      <td>0.100016</td>\n",
       "      <td>0.100006</td>\n",
       "      <td>0.599964</td>\n",
       "      <td>0.100007</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.014329</td>\n",
       "      <td>0.014291</td>\n",
       "      <td>0.014339</td>\n",
       "      <td>0.942726</td>\n",
       "      <td>0.014314</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.599984</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.100003</td>\n",
       "      <td>0.100005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.066671</td>\n",
       "      <td>0.066678</td>\n",
       "      <td>0.066671</td>\n",
       "      <td>0.733308</td>\n",
       "      <td>0.066672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.066673</td>\n",
       "      <td>0.066681</td>\n",
       "      <td>0.733302</td>\n",
       "      <td>0.066671</td>\n",
       "      <td>0.066673</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.599948</td>\n",
       "      <td>0.100024</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.100007</td>\n",
       "      <td>0.100011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.599984</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.100003</td>\n",
       "      <td>0.100005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.066671</td>\n",
       "      <td>0.066678</td>\n",
       "      <td>0.066671</td>\n",
       "      <td>0.733308</td>\n",
       "      <td>0.066672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.066673</td>\n",
       "      <td>0.066681</td>\n",
       "      <td>0.733302</td>\n",
       "      <td>0.066671</td>\n",
       "      <td>0.066673</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.599948</td>\n",
       "      <td>0.100024</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.100007</td>\n",
       "      <td>0.100011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic 0: [be, learn, see, say, psychology]  \\\n",
       "0                                     0.729750   \n",
       "1                                     0.025076   \n",
       "2                                     0.050738   \n",
       "3                                     0.839640   \n",
       "4                                     0.732786   \n",
       "5                                     0.033724   \n",
       "6                                     0.100009   \n",
       "7                                     0.839255   \n",
       "8                                     0.910858   \n",
       "9                                     0.050008   \n",
       "10                                    0.050321   \n",
       "11                                    0.067354   \n",
       "12                                    0.016796   \n",
       "13                                    0.033724   \n",
       "14                                    0.200000   \n",
       "15                                    0.011767   \n",
       "16                                    0.100007   \n",
       "17                                    0.014329   \n",
       "18                                    0.100004   \n",
       "19                                    0.066671   \n",
       "20                                    0.066673   \n",
       "21                                    0.599948   \n",
       "22                                    0.100004   \n",
       "23                                    0.066671   \n",
       "24                                    0.066673   \n",
       "25                                    0.599948   \n",
       "\n",
       "    Topic 1: [right, say, psychology, first, be]  \\\n",
       "0                                       0.066699   \n",
       "1                                       0.025006   \n",
       "2                                       0.050027   \n",
       "3                                       0.040015   \n",
       "4                                       0.066690   \n",
       "5                                       0.033344   \n",
       "6                                       0.100021   \n",
       "7                                       0.040013   \n",
       "8                                       0.022230   \n",
       "9                                       0.050019   \n",
       "10                                      0.050018   \n",
       "11                                      0.066688   \n",
       "12                                      0.016672   \n",
       "13                                      0.033345   \n",
       "14                                      0.200000   \n",
       "15                                      0.011770   \n",
       "16                                      0.100016   \n",
       "17                                      0.014291   \n",
       "18                                      0.599984   \n",
       "19                                      0.066678   \n",
       "20                                      0.066681   \n",
       "21                                      0.100024   \n",
       "22                                      0.599984   \n",
       "23                                      0.066678   \n",
       "24                                      0.066681   \n",
       "25                                      0.100024   \n",
       "\n",
       "    Topic 2: [be, still, go, minor, student]  \\\n",
       "0                                   0.066680   \n",
       "1                                   0.025002   \n",
       "2                                   0.050011   \n",
       "3                                   0.040006   \n",
       "4                                   0.066676   \n",
       "5                                   0.033493   \n",
       "6                                   0.100008   \n",
       "7                                   0.040588   \n",
       "8                                   0.022374   \n",
       "9                                   0.799153   \n",
       "10                                  0.798881   \n",
       "11                                  0.732291   \n",
       "12                                  0.933117   \n",
       "13                                  0.034437   \n",
       "14                                  0.200000   \n",
       "15                                  0.011784   \n",
       "16                                  0.100006   \n",
       "17                                  0.014339   \n",
       "18                                  0.100004   \n",
       "19                                  0.066671   \n",
       "20                                  0.733302   \n",
       "21                                  0.100010   \n",
       "22                                  0.100004   \n",
       "23                                  0.066671   \n",
       "24                                  0.733302   \n",
       "25                                  0.100010   \n",
       "\n",
       "    Topic 3: [structure, kind, student, science, interested]  \\\n",
       "0                                            0.066676          \n",
       "1                                            0.025002          \n",
       "2                                            0.799212          \n",
       "3                                            0.040196          \n",
       "4                                            0.066673          \n",
       "5                                            0.033336          \n",
       "6                                            0.100006          \n",
       "7                                            0.040004          \n",
       "8                                            0.022275          \n",
       "9                                            0.050572          \n",
       "10                                           0.050005          \n",
       "11                                           0.066673          \n",
       "12                                           0.016713          \n",
       "13                                           0.033762          \n",
       "14                                           0.200000          \n",
       "15                                           0.952905          \n",
       "16                                           0.599964          \n",
       "17                                           0.942726          \n",
       "18                                           0.100003          \n",
       "19                                           0.733308          \n",
       "20                                           0.066671          \n",
       "21                                           0.100007          \n",
       "22                                           0.100003          \n",
       "23                                           0.733308          \n",
       "24                                           0.066671          \n",
       "25                                           0.100007          \n",
       "\n",
       "    Topic 4: [good, first, move, lot, datum]  Polarity  Subjectivity  \\\n",
       "0                                   0.070196  0.910000      0.780000   \n",
       "1                                   0.899914  0.512121      0.551515   \n",
       "2                                   0.050012  0.500000      0.500000   \n",
       "3                                   0.040143  0.000000      0.000000   \n",
       "4                                   0.067176 -0.050000      0.050000   \n",
       "5                                   0.866102  0.500000      0.500000   \n",
       "6                                   0.599956  0.285714      0.535714   \n",
       "7                                   0.040141 -0.050000      0.200000   \n",
       "8                                   0.022262 -0.050000      0.200000   \n",
       "9                                   0.050248 -0.700000      0.666667   \n",
       "10                                  0.050775  0.500000      0.500000   \n",
       "11                                  0.066995  0.000000      0.000000   \n",
       "12                                  0.016702  0.201389      0.473611   \n",
       "13                                  0.864733  0.285714      0.535714   \n",
       "14                                  0.200000  0.000000      0.000000   \n",
       "15                                  0.011775  0.000000      0.050000   \n",
       "16                                  0.100007  0.250000      0.333333   \n",
       "17                                  0.014314  0.390625      0.500000   \n",
       "18                                  0.100005  0.000000      0.000000   \n",
       "19                                  0.066672  0.000000      0.000000   \n",
       "20                                  0.066673  0.200000      0.200000   \n",
       "21                                  0.100011  0.000000      0.000000   \n",
       "22                                  0.100005  0.000000      0.000000   \n",
       "23                                  0.066672  0.000000      0.000000   \n",
       "24                                  0.066673  0.200000      0.200000   \n",
       "25                                  0.100011  0.000000      0.000000   \n",
       "\n",
       "   Sentiment_Label  \n",
       "0         Positive  \n",
       "1         Positive  \n",
       "2         Positive  \n",
       "3          Neutral  \n",
       "4         Negative  \n",
       "5         Positive  \n",
       "6         Positive  \n",
       "7         Negative  \n",
       "8         Negative  \n",
       "9         Negative  \n",
       "10        Positive  \n",
       "11         Neutral  \n",
       "12        Positive  \n",
       "13        Positive  \n",
       "14         Neutral  \n",
       "15         Neutral  \n",
       "16        Positive  \n",
       "17        Positive  \n",
       "18         Neutral  \n",
       "19         Neutral  \n",
       "20        Positive  \n",
       "21         Neutral  \n",
       "22         Neutral  \n",
       "23         Neutral  \n",
       "24        Positive  \n",
       "25         Neutral  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to generate descriptive column names for topics in scikit-learn LDA\n",
    "def generate_topic_column_names(lda_model, vectorizer, num_top_words=5):\n",
    "    topic_column_names = []\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    for topic_id in range(lda_model.n_components):\n",
    "        # Get the top words for each topic based on the topic-word distribution\n",
    "        top_words_idx = lda_model.components_[topic_id].argsort()[:-num_top_words - 1:-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        \n",
    "        # Create a descriptive name like \"Topic X: [word1, word2, ...]\"\n",
    "        topic_name = f\"Topic {topic_id}: [\" + \", \".join(top_words) + \"]\"\n",
    "        topic_column_names.append(topic_name)\n",
    "    \n",
    "    return topic_column_names\n",
    "\n",
    "# Generate descriptive column names for topics\n",
    "topic_column_names = generate_topic_column_names(lda_model, vectorizer, num_top_words=5)\n",
    "\n",
    "# Convert topic distributions to a DataFrame for easier viewing\n",
    "topic_sentiment_df = pd.DataFrame(topic_distributions, columns=topic_column_names)\n",
    "\n",
    "# Append sentiment features to the topic DataFrame\n",
    "topic_sentiment_df[\"Polarity\"] = sentiment_df[\"Polarity\"]\n",
    "topic_sentiment_df[\"Subjectivity\"] = sentiment_df[\"Subjectivity\"]\n",
    "topic_sentiment_df[\"Sentiment_Label\"] = sentiment_df[\"Sentiment_Label\"]\n",
    "\n",
    "# Display the final DataFrame\n",
    "topic_sentiment_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_sentiment_df.drop('Sentiment_Label', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 0: [be, learn, see, say, psychology]</th>\n",
       "      <th>Topic 1: [right, say, psychology, first, be]</th>\n",
       "      <th>Topic 2: [be, still, go, minor, student]</th>\n",
       "      <th>Topic 3: [structure, kind, student, science, interested]</th>\n",
       "      <th>Topic 4: [good, first, move, lot, datum]</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.729750</td>\n",
       "      <td>0.066699</td>\n",
       "      <td>0.066680</td>\n",
       "      <td>0.066676</td>\n",
       "      <td>0.070196</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.025076</td>\n",
       "      <td>0.025006</td>\n",
       "      <td>0.025002</td>\n",
       "      <td>0.025002</td>\n",
       "      <td>0.899914</td>\n",
       "      <td>0.512121</td>\n",
       "      <td>0.551515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.050738</td>\n",
       "      <td>0.050027</td>\n",
       "      <td>0.050011</td>\n",
       "      <td>0.799212</td>\n",
       "      <td>0.050012</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.839640</td>\n",
       "      <td>0.040015</td>\n",
       "      <td>0.040006</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>0.040143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.732786</td>\n",
       "      <td>0.066690</td>\n",
       "      <td>0.066676</td>\n",
       "      <td>0.066673</td>\n",
       "      <td>0.067176</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.033724</td>\n",
       "      <td>0.033344</td>\n",
       "      <td>0.033493</td>\n",
       "      <td>0.033336</td>\n",
       "      <td>0.866102</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.100009</td>\n",
       "      <td>0.100021</td>\n",
       "      <td>0.100008</td>\n",
       "      <td>0.100006</td>\n",
       "      <td>0.599956</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.535714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.839255</td>\n",
       "      <td>0.040013</td>\n",
       "      <td>0.040588</td>\n",
       "      <td>0.040004</td>\n",
       "      <td>0.040141</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.910858</td>\n",
       "      <td>0.022230</td>\n",
       "      <td>0.022374</td>\n",
       "      <td>0.022275</td>\n",
       "      <td>0.022262</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.050008</td>\n",
       "      <td>0.050019</td>\n",
       "      <td>0.799153</td>\n",
       "      <td>0.050572</td>\n",
       "      <td>0.050248</td>\n",
       "      <td>-0.700000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.050321</td>\n",
       "      <td>0.050018</td>\n",
       "      <td>0.798881</td>\n",
       "      <td>0.050005</td>\n",
       "      <td>0.050775</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.067354</td>\n",
       "      <td>0.066688</td>\n",
       "      <td>0.732291</td>\n",
       "      <td>0.066673</td>\n",
       "      <td>0.066995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.016796</td>\n",
       "      <td>0.016672</td>\n",
       "      <td>0.933117</td>\n",
       "      <td>0.016713</td>\n",
       "      <td>0.016702</td>\n",
       "      <td>0.201389</td>\n",
       "      <td>0.473611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.033724</td>\n",
       "      <td>0.033345</td>\n",
       "      <td>0.034437</td>\n",
       "      <td>0.033762</td>\n",
       "      <td>0.864733</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.535714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.011767</td>\n",
       "      <td>0.011770</td>\n",
       "      <td>0.011784</td>\n",
       "      <td>0.952905</td>\n",
       "      <td>0.011775</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.100007</td>\n",
       "      <td>0.100016</td>\n",
       "      <td>0.100006</td>\n",
       "      <td>0.599964</td>\n",
       "      <td>0.100007</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.014329</td>\n",
       "      <td>0.014291</td>\n",
       "      <td>0.014339</td>\n",
       "      <td>0.942726</td>\n",
       "      <td>0.014314</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.599984</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.100003</td>\n",
       "      <td>0.100005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.066671</td>\n",
       "      <td>0.066678</td>\n",
       "      <td>0.066671</td>\n",
       "      <td>0.733308</td>\n",
       "      <td>0.066672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.066673</td>\n",
       "      <td>0.066681</td>\n",
       "      <td>0.733302</td>\n",
       "      <td>0.066671</td>\n",
       "      <td>0.066673</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.599948</td>\n",
       "      <td>0.100024</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.100007</td>\n",
       "      <td>0.100011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.599984</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.100003</td>\n",
       "      <td>0.100005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.066671</td>\n",
       "      <td>0.066678</td>\n",
       "      <td>0.066671</td>\n",
       "      <td>0.733308</td>\n",
       "      <td>0.066672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.066673</td>\n",
       "      <td>0.066681</td>\n",
       "      <td>0.733302</td>\n",
       "      <td>0.066671</td>\n",
       "      <td>0.066673</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.599948</td>\n",
       "      <td>0.100024</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.100007</td>\n",
       "      <td>0.100011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic 0: [be, learn, see, say, psychology]  \\\n",
       "0                                     0.729750   \n",
       "1                                     0.025076   \n",
       "2                                     0.050738   \n",
       "3                                     0.839640   \n",
       "4                                     0.732786   \n",
       "5                                     0.033724   \n",
       "6                                     0.100009   \n",
       "7                                     0.839255   \n",
       "8                                     0.910858   \n",
       "9                                     0.050008   \n",
       "10                                    0.050321   \n",
       "11                                    0.067354   \n",
       "12                                    0.016796   \n",
       "13                                    0.033724   \n",
       "14                                    0.200000   \n",
       "15                                    0.011767   \n",
       "16                                    0.100007   \n",
       "17                                    0.014329   \n",
       "18                                    0.100004   \n",
       "19                                    0.066671   \n",
       "20                                    0.066673   \n",
       "21                                    0.599948   \n",
       "22                                    0.100004   \n",
       "23                                    0.066671   \n",
       "24                                    0.066673   \n",
       "25                                    0.599948   \n",
       "\n",
       "    Topic 1: [right, say, psychology, first, be]  \\\n",
       "0                                       0.066699   \n",
       "1                                       0.025006   \n",
       "2                                       0.050027   \n",
       "3                                       0.040015   \n",
       "4                                       0.066690   \n",
       "5                                       0.033344   \n",
       "6                                       0.100021   \n",
       "7                                       0.040013   \n",
       "8                                       0.022230   \n",
       "9                                       0.050019   \n",
       "10                                      0.050018   \n",
       "11                                      0.066688   \n",
       "12                                      0.016672   \n",
       "13                                      0.033345   \n",
       "14                                      0.200000   \n",
       "15                                      0.011770   \n",
       "16                                      0.100016   \n",
       "17                                      0.014291   \n",
       "18                                      0.599984   \n",
       "19                                      0.066678   \n",
       "20                                      0.066681   \n",
       "21                                      0.100024   \n",
       "22                                      0.599984   \n",
       "23                                      0.066678   \n",
       "24                                      0.066681   \n",
       "25                                      0.100024   \n",
       "\n",
       "    Topic 2: [be, still, go, minor, student]  \\\n",
       "0                                   0.066680   \n",
       "1                                   0.025002   \n",
       "2                                   0.050011   \n",
       "3                                   0.040006   \n",
       "4                                   0.066676   \n",
       "5                                   0.033493   \n",
       "6                                   0.100008   \n",
       "7                                   0.040588   \n",
       "8                                   0.022374   \n",
       "9                                   0.799153   \n",
       "10                                  0.798881   \n",
       "11                                  0.732291   \n",
       "12                                  0.933117   \n",
       "13                                  0.034437   \n",
       "14                                  0.200000   \n",
       "15                                  0.011784   \n",
       "16                                  0.100006   \n",
       "17                                  0.014339   \n",
       "18                                  0.100004   \n",
       "19                                  0.066671   \n",
       "20                                  0.733302   \n",
       "21                                  0.100010   \n",
       "22                                  0.100004   \n",
       "23                                  0.066671   \n",
       "24                                  0.733302   \n",
       "25                                  0.100010   \n",
       "\n",
       "    Topic 3: [structure, kind, student, science, interested]  \\\n",
       "0                                            0.066676          \n",
       "1                                            0.025002          \n",
       "2                                            0.799212          \n",
       "3                                            0.040196          \n",
       "4                                            0.066673          \n",
       "5                                            0.033336          \n",
       "6                                            0.100006          \n",
       "7                                            0.040004          \n",
       "8                                            0.022275          \n",
       "9                                            0.050572          \n",
       "10                                           0.050005          \n",
       "11                                           0.066673          \n",
       "12                                           0.016713          \n",
       "13                                           0.033762          \n",
       "14                                           0.200000          \n",
       "15                                           0.952905          \n",
       "16                                           0.599964          \n",
       "17                                           0.942726          \n",
       "18                                           0.100003          \n",
       "19                                           0.733308          \n",
       "20                                           0.066671          \n",
       "21                                           0.100007          \n",
       "22                                           0.100003          \n",
       "23                                           0.733308          \n",
       "24                                           0.066671          \n",
       "25                                           0.100007          \n",
       "\n",
       "    Topic 4: [good, first, move, lot, datum]  Polarity  Subjectivity  \n",
       "0                                   0.070196  0.910000      0.780000  \n",
       "1                                   0.899914  0.512121      0.551515  \n",
       "2                                   0.050012  0.500000      0.500000  \n",
       "3                                   0.040143  0.000000      0.000000  \n",
       "4                                   0.067176 -0.050000      0.050000  \n",
       "5                                   0.866102  0.500000      0.500000  \n",
       "6                                   0.599956  0.285714      0.535714  \n",
       "7                                   0.040141 -0.050000      0.200000  \n",
       "8                                   0.022262 -0.050000      0.200000  \n",
       "9                                   0.050248 -0.700000      0.666667  \n",
       "10                                  0.050775  0.500000      0.500000  \n",
       "11                                  0.066995  0.000000      0.000000  \n",
       "12                                  0.016702  0.201389      0.473611  \n",
       "13                                  0.864733  0.285714      0.535714  \n",
       "14                                  0.200000  0.000000      0.000000  \n",
       "15                                  0.011775  0.000000      0.050000  \n",
       "16                                  0.100007  0.250000      0.333333  \n",
       "17                                  0.014314  0.390625      0.500000  \n",
       "18                                  0.100005  0.000000      0.000000  \n",
       "19                                  0.066672  0.000000      0.000000  \n",
       "20                                  0.066673  0.200000      0.200000  \n",
       "21                                  0.100011  0.000000      0.000000  \n",
       "22                                  0.100005  0.000000      0.000000  \n",
       "23                                  0.066672  0.000000      0.000000  \n",
       "24                                  0.066673  0.200000      0.200000  \n",
       "25                                  0.100011  0.000000      0.000000  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'topic_sentiment_df' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "topic_sentiment_df.to_csv('topic_sentiment_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('text': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed6dd6554ff284cdadabcc502f62466567fa34075a8b2d6a902d3757ebd4651f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
